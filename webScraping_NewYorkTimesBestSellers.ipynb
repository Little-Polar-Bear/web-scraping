{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d976614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bae6bf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base url that is used in all of the pages, format inbetween curly braces with relative path.\n",
    "base_url = \"https://www.nytimes.com/books/best-sellers/{}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbcde4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the part of the url that points to the exact web page.\n",
    "relative_url = \"\"\n",
    "# string list of relative url paths\n",
    "urls = []\n",
    "urls.insert(0, \"combined-print-and-e-book-fiction\")\n",
    "urls.insert(1, \"combined-print-and-e-book-nonfiction\")\n",
    "urls.insert(2, \"hardcover-fiction\")\n",
    "urls.insert(3, \"hardcover-nonfiction\")\n",
    "urls.insert(4, \"trade-fiction-paperback\")\n",
    "urls.insert(5, \"paperback-nonfiction\")\n",
    "urls.insert(6, \"advice-how-to-and-miscellaneous\")\n",
    "urls.insert(7, \"childrens-middle-grade-hardcover\")\n",
    "urls.insert(8, \"picture-books\")\n",
    "urls.insert(9, \"series-books\")\n",
    "urls.insert(10, \"young-adult-hardcover\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "577d7670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# = note\n",
    "#* = soup method call\n",
    "\n",
    "# individual page header, included is when the week finishes.\n",
    "#* soup.select(\".css-68u1tu\")\n",
    "\n",
    "# title\n",
    "#* soup.select(\"ol > li > article > div > a > h3\")\n",
    "\n",
    "# number of weeks in list\n",
    "#* soup.select(\"ol > li > article > div > a > p\")[0].text\n",
    "\n",
    "# author\n",
    "#* soup.select(\"ol > li > article > div > a > p\")[1].text\n",
    "\n",
    "# publisher\n",
    "#* soup.select(\"ol > li > article > div > a > p\")[2].text\n",
    "\n",
    "# description\n",
    "#* soup.select(\"ol > li > article > div > a > p\")[3].text\n",
    "\n",
    "# links to buy book\n",
    "#* soup.select(\"ol > li > article > div > div > div > div > ul > li > a\")[0]['href']\n",
    "\n",
    "# link text title as they appear\n",
    "#* soup.select(\"ol > li > article > div > div > div > div > ul > li > a\")[0].text\n",
    "\n",
    "# image url\n",
    "#* soup.select(\"ol > li > article > footer > div > a > img\")[0]['src']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83f33168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing...combined-print-and-e-book-fiction\n",
      "finished and saved as json file...combined-print-and-e-book-fiction_bestSellingBooks.json\n",
      "Waiting here for 15 seconds to avoid bot detection\n",
      "Executing...combined-print-and-e-book-nonfiction\n",
      "finished and saved as json file...combined-print-and-e-book-nonfiction_bestSellingBooks.json\n",
      "Waiting here for 15 seconds to avoid bot detection\n",
      "Executing...hardcover-fiction\n",
      "finished and saved as json file...hardcover-fiction_bestSellingBooks.json\n",
      "Waiting here for 15 seconds to avoid bot detection\n",
      "Executing...hardcover-nonfiction\n",
      "finished and saved as json file...hardcover-nonfiction_bestSellingBooks.json\n",
      "Waiting here for 15 seconds to avoid bot detection\n",
      "Executing...trade-fiction-paperback\n",
      "finished and saved as json file...trade-fiction-paperback_bestSellingBooks.json\n",
      "Waiting here for 15 seconds to avoid bot detection\n",
      "Executing...paperback-nonfiction\n",
      "finished and saved as json file...paperback-nonfiction_bestSellingBooks.json\n",
      "Waiting here for 15 seconds to avoid bot detection\n",
      "Executing...advice-how-to-and-miscellaneous\n",
      "finished and saved as json file...advice-how-to-and-miscellaneous_bestSellingBooks.json\n",
      "Waiting here for 15 seconds to avoid bot detection\n",
      "Executing...childrens-middle-grade-hardcover\n",
      "finished and saved as json file...childrens-middle-grade-hardcover_bestSellingBooks.json\n",
      "Waiting here for 15 seconds to avoid bot detection\n",
      "Executing...picture-books\n",
      "finished and saved as json file...picture-books_bestSellingBooks.json\n",
      "Waiting here for 15 seconds to avoid bot detection\n",
      "Executing...series-books\n",
      "finished and saved as json file...series-books_bestSellingBooks.json\n",
      "Waiting here for 15 seconds to avoid bot detection\n",
      "Executing...young-adult-hardcover\n",
      "finished and saved as json file...young-adult-hardcover_bestSellingBooks.json\n",
      "Waiting here for 15 seconds to avoid bot detection\n",
      "Loop Finished\n"
     ]
    }
   ],
   "source": [
    "# iterate through all list items using the above soup method calls and save as a json file format.\n",
    "\n",
    "#create the object list we will use to append each object of the key_value_pairs to:\n",
    "key_value_pairs_list = []\n",
    "\n",
    "# create a header to use with requests, so the web page does not block us as a robot.\n",
    "headers = requests.utils.default_headers()\n",
    "headers.update({\n",
    "    'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',\n",
    "})\n",
    "\n",
    "for string in urls:\n",
    "    print(\"Executing...\" + string)\n",
    "    #create the master_list we will use to append the each list of key_value_pairs to:\n",
    "    master_list = []\n",
    "    \n",
    "    #set the relative_url_string\n",
    "    relative_url = string\n",
    "    \n",
    "    # open portal to url\n",
    "    res = requests.get(base_url.format(relative_url), headers=headers)\n",
    "    \n",
    "    # create the soup\n",
    "    soup = bs4.BeautifulSoup(res.text, \"lxml\")\n",
    "    \n",
    "    #get the catergory header for the list of books.\n",
    "    category_header = soup.select(\".css-68u1tu\")[0].text\n",
    "\n",
    "    #get all the books in the list. this class call points to the first ordered list in the web-page.\n",
    "    list_items = soup.select(\".css-12yzwg4 > li > article\")\n",
    "\n",
    "    #int value used for setting the rank of the book in the list.\n",
    "    list_rank = 0\n",
    "\n",
    "    #for each item in the list, exctract the relevant information:\n",
    "    for item in list_items:\n",
    "        try:\n",
    "            #set the book rank\n",
    "            list_rank = list_rank + 1\n",
    "            \n",
    "            #get book title.\n",
    "            if (item.select(\"div > a > h3\") != []):\n",
    "                title = item.select(\"div > a > h3\")[0].text\n",
    "            #get author, number of weeks in the list, publisher and description.\n",
    "            if (item.select(\"div > a > p\") != []):\n",
    "                weeks_in_list = item.select(\"div > a > p\")[0].text\n",
    "                author = item.select(\"div > a > p\")[1].text\n",
    "                publisher = item.select(\"div > a > p\")[2].text\n",
    "                description = item.select(\"div > a > p\")[3].text\n",
    "            #get the store text and link url to buy book.\n",
    "            if (item.select(\"div > div > div > div > ul > li > a\") != []):\n",
    "                #store each set of store name and adresses for each book in an array list.\n",
    "                # create the array\n",
    "                purchase_websites_list = []\n",
    "                                \n",
    "                for link in (item.select(\"div > div > div > div > ul > li > a\")):\n",
    "                    #create review_quotes text initiated to 'No reviews found'\n",
    "                    #(in case there are none).\n",
    "                    #review_quotes = \"No reviews found :(\"\n",
    "                    # get links and store name\n",
    "                    website_name = link.text\n",
    "                    website_link = link['href']\n",
    "                        \n",
    "                    # create the object\n",
    "                    purchase_websites_object = {}\n",
    "                    #set the key value pairs\n",
    "                    purchase_websites_object[\"storeName\"] = website_name\n",
    "                    purchase_websites_object[\"webAdress\"] = website_link\n",
    "                    # add the key value pairs to the array\n",
    "                    purchase_websites_list.append(purchase_websites_object)\n",
    "                    \n",
    "            #get the image url for each book.\n",
    "            if (item.select(\"footer > div > a > img\")):\n",
    "                image_url = item.select(\"footer > div > a > img\")[0]['src']\n",
    "        \n",
    "            #create a key_value_pairs list object\n",
    "            key_value_pairs = {}\n",
    "        \n",
    "            # set the key value pair attributes.\n",
    "            key_value_pairs['title'] = title\n",
    "            key_value_pairs['author'] = author\n",
    "            key_value_pairs['description'] = description\n",
    "            key_value_pairs['publisher'] = publisher\n",
    "            key_value_pairs['weeks_in_list'] = weeks_in_list\n",
    "            key_value_pairs['image_url'] = image_url\n",
    "            key_value_pairs['list_rank'] = list_rank\n",
    "            key_value_pairs['purchase_websites'] = purchase_websites_list\n",
    "        \n",
    "            #append the key_values_pairs object to the key_value_pairs list \n",
    "            key_value_pairs_list.append(key_value_pairs)\n",
    "        \n",
    "        except AttributeError as ex:\n",
    "            print('Error:', ex)\n",
    "        \n",
    "    # create a new master object to append each key_values_pair list \n",
    "    # along with the asscociated category header.\n",
    "    master_object = {}\n",
    "\n",
    "    #set the category header in the newly created master_object\n",
    "    master_object[\"categoryHeader\"] = category_header\n",
    "    master_object[\"listItems\"] = key_value_pairs_list\n",
    "\n",
    "    #append master_object to the master_list\n",
    "    master_list.append(master_object)\n",
    "    \n",
    "    # create a new list for the next category to save to.\n",
    "    key_value_pairs_list = []\n",
    "    \n",
    "    #create the object json will convert\n",
    "    convert_object = {}\n",
    "    convert_object['contentList'] = master_list\n",
    "\n",
    "    #convert master_list into a json file format and save file:\n",
    "    f = open(string + '_bestSellingBooks.json', 'w')\n",
    "    f.write(json.dumps(convert_object))\n",
    "    f.close()\n",
    "    print(\"finished and saved as json file...\" + string + '_bestSellingBooks.json')\n",
    "    print(\"Waiting here for 15 seconds to avoid bot detection\")\n",
    "    time.sleep(15)\n",
    " \n",
    " # after loop \n",
    "print(\"Loop Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aac817",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
